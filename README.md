# group_recommender
### Как я понимаю что происходит в алгоритме [отсюда](https://arxiv.org/pdf/1003.0146.pdf)
[здесь ещё тоже хорошо описано все](http://john-maxwell.com/post/2017-03-17/)

У нас есть 2 вида контекстов:
* контекст по пользователям
* контекст по группам 

### Как мы ими распоряжаемся

Контекст по пользователям используется непосредственно при подсчете *выплат* для рук при определении какую из них выдать пользователю, а также при обновлении матрицы соответствующей группы если результат выдачи совпал с выбором пользователя (строчки **8**, **9**, и, если все хорошо, то **12**, **13** из **Algortithm 1** (переменная x_t_a)

С контекстом по группам интереснее. Опять же, исходя из статьи, они предлагают в качестве матрицы *D_a* использовать уже ранее полученные наблюдения с контекстами пользователей (например, вот есть *m* пользователей, которые кликнули на группу, и каждый описан вектором размерности *d*.И предлагается из этой матрицы получить матрицу размерности *d* на *d* транспонированием и соответвствующим умножением и дальше уже ее использовать в гребневой регрессии и т.д.).

Если же таковых нет, т.е. группа (в их случае статья) не встречалась, то вот эту матрицу сделать просто единичной и дальше проводить расчёты согласно алгоритма. 

Но, как я понял, в нашем случае у каждой группы уже есть какой-то заранее подсчитанный контекст в виде как раз вот того набора данных по группам. И хочется вот ту матрицу получить из этого вектора, а не просто использовать скучную единичную матрицу. 

Если же с контекстом группы тоже беда (например, группа новая или малоактивная), то тогда остается только использовать единичную матрицу.

### Идеи по проверке работоспособности

Как я уже говорил, у меня все подает по памяти уже на этапе инициализации (у меня трехмерная матрица для групп получается размерности 33578 на 400 с копейками на 400 с копейками и ему(ноуту) уже сложно хранить столько единичных матриц, не говоря уже про их обращения)

Я хотел попробоавать запустить на небольшой выборке групп, скажем 100 - 200 штук. И для наглядности хочу выбрать эти группы так, чтобы их суммарное количество показов пользователям было максимальным. Я уже пробовал выбрать 200 **случайных** групп и у меня получалось порядка 60 000 событий, чего, наверное, для проверки жизнеспособности должно хватить. На максимальных 200 группах будет, я думаю порядка 100 000 - 150 000 показов.

Надеюсь ничего в таком случае не упадет и будет работать разумное количество времени.


### Неразрешенные вопросы (может обновляться)

* Что делать с *r_t* из статьи (real-time payoff)? Есть два варианта: либо мы штрафуем за ошибки, и тогда *r_t* берется из {-1, 1}. Либо мы более гуманно относимся к этому, и тогда *r_t* уже из {0, 1}
